{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HF-format dataset from a file directory with audios and transcriptions and then uploading it to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U datasets\n",
    "# Need to install datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "from datasets import load_dataset, Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up folder names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"<main_path>\" # main path to the folder with both audio files and transcriptions\n",
    "\n",
    "audio_folder = \"audio\" # name of the folder with audio files\n",
    "transcriptions_folder = \"transcriptions\" # name of the folder with transcriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: both audio and transcriptions folder should contain .wav and .txt files respectively in the same subfolders as in the example below:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \\audio\n",
    "        \\lectures\n",
    "            \\lecture_1.wav\n",
    "            ...\n",
    "            \\lecture_10.wav\n",
    "        \\seminars\n",
    "            \\seminar_1.wav\n",
    "            ...\n",
    "            \\seminar_20.wav\n",
    "        \\tutorials\n",
    "            \\tutorial_1.wav\n",
    "            ...\n",
    "            \\tutorial_123.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \\transcriptions\n",
    "        \\lectures\n",
    "            \\lecture_1.txt\n",
    "            ...\n",
    "            \\lecture_10.txt\n",
    "        \\seminars\n",
    "            \\seminar_1.txt\n",
    "            ...\n",
    "            \\seminar_20.txt\n",
    "        \\tutorials\n",
    "            \\tutorial_1.txt\n",
    "            ...\n",
    "            \\tutorial_123.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_ds = load_dataset(\n",
    "    \"audiofolder\",\n",
    "    data_dir=path.join(main_path, audio_folder)\n",
    ").cast_column(\n",
    "    \"audio\",\n",
    "    Audio(                      # Need to cast all audio files to mono 16kHz\n",
    "        sampling_rate=16_000,   # since most ASR models are trained on 16kHz audio\n",
    "        mono=True,              # and do not support any other sampling rates\n",
    "    ),\n",
    ")\n",
    "\n",
    "transcriptions_ds = load_dataset(path.join(main_path, transcriptions_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding transcriptions to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = audio_ds[\"train\"].add_column(\n",
    "    name=\"transcription\",\n",
    "    column=transcriptions_ds[\"train\"][\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = full_ds.train_test_split(\n",
    "    test_size=0.2, # Let test set be 20% of the data\n",
    "    stratify_by_column=\"label\", # Make sure split saves the label proportions\n",
    "    shuffle=True,\n",
    "    seed=42, # Just for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds[\"train\"][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the dataset to Hugging Face repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"<HF_TOKEN>\" # Replace with your token !!! (it must have write permissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Optionally, you can log in to your Hugging Face account using the `huggingface-cli login` command and omit the `token` argument._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"<user_name>/<repository_name>\" # Dataset repository name\n",
    "\n",
    "full_ds.push_to_hub(\n",
    "    dataset_name,\n",
    "    private=True,\n",
    "    max_shard_size=\"300MB\", # Splits the dataset into shards of 300MB each to avoid errors on uploading\n",
    "    token=hf_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data and check its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][42]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
